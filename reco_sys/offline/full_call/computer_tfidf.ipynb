{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "PYSPARK_PYTHON = \"/opt/anaconda3/envs/rec_sys/bin/python\"\n",
    "\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON\n",
    "\n",
    "from offline import SparkSessionBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordsToTfidf(SparkSessionBase):\n",
    "    \n",
    "    SPARK_APP_NAME = \"keywordsByTFIDF\"\n",
    "    SPARK_EXECUTOR_MEMORY=\"4g\"\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.spark = self._create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ktt = KeywordsToTfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ktt.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktt.spark.sql(\"use article\")\n",
    "article_data = ktt.spark.sql(\"select * from article_data limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "global stopwords_list\n",
    "stopwords_path = \"/home/data/etc/stop_words.txt\"\n",
    "stopwords_list = [i.strip() for i in codecs.open(stopwords_path).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(partition):\n",
    "    import jieba\n",
    "    import os\n",
    "    import jieba.posseg as pseg\n",
    "    import re\n",
    "    import codecs\n",
    "     \n",
    "    # 获取停用词\n",
    "#     def get_stopwords_list():\n",
    "#         \"\"\"返回stopwords列表\"\"\"\n",
    "#         stopwords_path = \"/home/data/etc/stop_words.txt\"\n",
    "#         stopwords_list = [i.strip() for i in codecs.open(stopwords_path,'w').readlines()]\n",
    "        \n",
    "#     # 所有的停用词列表\n",
    "#     stopwords_list = get_stopwords_list()\n",
    "    \n",
    "    # 分词\n",
    "    def cut_sentence(sentence):\n",
    "        \"\"\"对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词\"\"\"\n",
    "        # print(sentence,\"*\"*100)\n",
    "        # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]\n",
    "        seg_list = pseg.lcut(sentence)\n",
    "        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n",
    "        filtered_words_list = []\n",
    "        for seg in seg_list:\n",
    "            # print(seg)\n",
    "            if len(seg.word) <= 1:\n",
    "                continue\n",
    "            elif seg.flag == \"eng\":\n",
    "                if len(seg.word) <= 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_words_list.append(seg.word)\n",
    "            elif seg.flag.startswith(\"n\"):\n",
    "                filtered_words_list.append(seg.word)\n",
    "            elif seg.flag in [\"x\", \"eng\"]:  # 是自定一个词语或者是英文单词\n",
    "                filtered_words_list.append(seg.word)\n",
    "        return filtered_words_list\n",
    "    \n",
    "    def cut_sentence_2(sentence):\n",
    "        \"\"\"对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词\"\"\"\n",
    "        # print(sentence,\"*\"*100)\n",
    "        # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]\n",
    "        postag = ['nr','ns','nt','nx','nz','n']\n",
    "        pseg = pkuseg.pkuseg(postag = True)\n",
    "        seg_list = pseg.cut(sentence)\n",
    "        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n",
    "        filtered_words_list = []\n",
    "        for seg in seg_list:\n",
    "            try:\n",
    "                if len(seg[0]) <= 1:\n",
    "                    continue\n",
    "                elif seg[-1] in postag:\n",
    "                    if len(seg[0]) <= 2:\n",
    "                        continue\n",
    "                    else:\n",
    "                        filtered_words_list.append(seg[0])\n",
    "            except Exception as e:\n",
    "                print(\"you have a error is {},the data is {}\".format(e, seg))\n",
    "        return filtered_words_list\n",
    "    \n",
    "    for row in partition:\n",
    "        sentence = re.sub(\"<.*?>\", \"\", row.sentence)    # 替换掉标签数据\n",
    "        words = cut_sentence(sentence)\n",
    "        yield row.article_id, row.channel_name, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------------------------------------+-------------------------------------+------------------------+-----------------------------+-----------------------------+\n",
      "|          article_id|channel_name|                                   title|                              content|                  source|                  source_tags|                     sentence|\n",
      "+--------------------+------------+----------------------------------------+-------------------------------------+------------------------+-----------------------------+-----------------------------+\n",
      "|00000982fda1948f4...|      投资界|     新能源汽车10大趋势预测：商用车率...|编者按：能源危机愈演愈烈，将新能源...|微信公众号：品途商业评论|    [ \"汽车\" , \"新能源\" , ...|    [ \"汽车\" , \"新能源\" , ...|\n",
      "|000039bca765fca66...|      猎云网|    成都监察委立案调查东方广益6亿投资...| 猎云网：成都成华区监察委立案调查“...|                  unknow|  [ \"罗永浩\" , \"锤子科技\" ...|  [ \"罗永浩\" , \"锤子科技\" ...|\n",
      "|0000b6f5182e097e8...|      砍柴网|      腾不出手来管理二手车业务   特斯...|    11月20日消息，据两位知情人士透...|                  unknow|   [ \"特斯拉\" , \"二手车业务\"]| [ \"特斯拉\" , \"二手车业务\"...|\n",
      "|0000bca75b0386b06...|      希鸥网|       成立4年获4轮融资，覆盖用户超10...|   快牛金科创始人兼CEO倪抒音艾瑞咨...|                  unknow|                       unknow|  unknow,希鸥网,成立4年获4...|\n",
      "|0000f7b314fe7970d...|        36氪|           谷歌母公司Alphabet研发智能...|      据外媒CNBC报道，谷歌母公司Al...|                  unknow|                       unknow|   unknow,36氪,谷歌母公司A...|\n",
      "|00019c1868a4ae670...|      投资界|   著名经济专家荟聚雅居乐地产雅尊会，...|    8月10日，由雅居乐地产雅尊会（T...|              投资界综合|  [ \"雅居乐\" , \"消费升级\" ...|  [ \"雅居乐\" , \"消费升级\" ...|\n",
      "|0001a643f8a623a0c...|      希鸥网|   希鸥网专访阿拉丁智店：以大数据和人...|    据世界银行最新发布的《2019年营...|                  unknow|                       unknow|unknow,希鸥网,希鸥网专访阿...|\n",
      "|0001c67ae9c818e47...|      猎云网|          集成式网络搜索平台Journal获...|       【猎云网（微信号：ilieyun）...|                  unknow|       [ \"Journal\" , \"搜索...|       [ \"Journal\" , \"搜索...|\n",
      "|0002d00a0273e73c8...|        亿欧|黄石头：公立医院的医生，是时候未雨绸缪了|【编者按】种种迹象表明，公立医院正...|                新医频道| [ \"公立医院改革\" , \"医生\"...| [ \"公立医院改革\" , \"医生\"...|\n",
      "|000314168bdf88130...|      投资界|         6000亿元新蓝海，37度要用科技...| “目前市面上，所有关于家的智能产品...|              投资界综合|  [ \"37度智能家具\" , \"照明...|  [ \"37度智能家具\" , \"照明...|\n",
      "|00033465e6868fcbf...|        亿欧|      苏宁物流推出“超级准时达”，全天1...|五一假期刚过，苏宁物流就为用户带来...|                    亿欧|  [ \"苏宁物流\" , \"准时达\" ...|  [ \"苏宁物流\" , \"准时达\" ...|\n",
      "|0003d40b95a019c7d...|      投资界|          投资界快讯|PKFARE（比客）完...|            投资界（ID：pedaily201...|                  投资界|     [ \"比客\" , \"左驭\" , \"...|     [ \"比客\" , \"左驭\" , \"...|\n",
      "|00047e8d4942862a8...| 新芽NewSeed|                  为什么“拟人”并非AI正途|目前，围绕人工智能对人类的模仿，在...|      微信公众号：猎云网|[ \"人工智能\" , \"自动化设置...|[ \"人工智能\" , \"自动化设置...|\n",
      "|00049e1b615f017e9...|        亿欧|     图灵奖得主对话微软沈向洋：AI不是...|       9月18日亿欧华东消息，2018世...|                    亿欧|     [ \"微软\" , \"小冰\" , \"...|     [ \"微软\" , \"小冰\" , \"...|\n",
      "|0004f0d046da399e6...|        亿欧|          将被自动驾驶技术改变的九大行业|【编者按】随着科技的日新月异，自动...|                盖世汽车| [ \"自动驾驶\" , \"无人驾驶\"...| [ \"自动驾驶\" , \"无人驾驶\"...|\n",
      "|0004f297f3e596aa0...|        亿欧|   数百家公司竞争少儿编程赛道，什么样...|     【编者按】政策鼓励，STEAM教育...|                幼教观察|   [ \"少儿编程\" , \"奥数\" ,...|   [ \"少儿编程\" , \"奥数\" ,...|\n",
      "|0005bc35146c88a95...|        亿欧|     将终结宁德“时代”的，也许不是比亚...|    【编者按】2019，是影响我国新能...|                    亿欧|  [ \"宁德时代\" , \"比亚迪\" ...|  [ \"宁德时代\" , \"比亚迪\" ...|\n",
      "|000676ddec6a8b1eb...|        36氪|         以太坊的PoS之路：最后阶段Ser...|  编者按：本文转载自“巴比特资讯”，...|                  unknow|                       unknow|    unknow,36氪,以太坊的Po...|\n",
      "|0006d4a4940e140ec...|        36氪|             36氪首发 | 用B2B2C模式切...|      36氪获悉，新高考排选课SaaS服...|                  unknow|                       unknow|     unknow,36氪,36氪首发 ...|\n",
      "|000782d1d98bc6827...|        亿欧|       首席出行要闻丨端午客运量达1.34...|交通运输部：端午假期全国道路运输客...|                亿欧汽车|      [ \"特斯拉\" , \"ofo\" ,...|      [ \"特斯拉\" , \"ofo\" ,...|\n",
      "+--------------------+------------+----------------------------------------+-------------------------------------+------------------------+-----------------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words_df = article_data.rdd.mapPartitions(segmentation).toDF([\"article_id\", \"channel_name\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.registerTempTable(\"templetable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------------------------+\n",
      "|          article_id|channel_name|                        words|\n",
      "+--------------------+------------+-----------------------------+\n",
      "|0ae5ec196aa06c47b...|      品途网| [阿里, 智能, 汽车, 品途, ...|\n",
      "|0ae61c67f4510fc0d...|      DoNews|         [unknow, DoNews, ...|\n",
      "|0ae64ae3d5efd8db2...|      鞭牛士| [神舟, 牛士, 倾心, 神舟, ...|\n",
      "|0ae75ea628ffdc7ee...|        36氪|    [unknow, 埃森哲, 鼻祖,...|\n",
      "|0ae7a7499baea87bf...|    站长之家| [斗鱼, 宣告, 斗鱼, 家暴, ...|\n",
      "|0ae7f35879dca0207...|      品途网| [手游, 游戏, 公司, 魔兽, ...|\n",
      "|0ae87d308dc7f2229...|      思达派|[思达, 顶级, 机构, 专业化,...|\n",
      "|0ae89a55248c00603...|        亿欧|   [ofo, 摩拜, 戴威, 黄车,...|\n",
      "|0ae8deaf40fcaab1d...| HRTechChina|[资讯, 人力资源, 实验室, L...|\n",
      "|0ae8e8459a65817b6...|  世界经理人|[哲学, 概念, 企业家, 企业,...|\n",
      "+--------------------+------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql(\"select * from templetable limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"countFeatures\", vocabSize=200*10000, minDF=1.0)\n",
    "# 训练词频统计模型\n",
    "cv_model = cv.fit(words_df)\n",
    "cv_model.write().overwrite().save(\"hdfs://192.168.0.52:8020/models/CV.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-42027410d4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://192.168.0.52:8020/models/CV.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 得出词频向量结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcv_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 训练IDF模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "cv_model = CountVectorizerModel.load(\"hdfs://192.168.0.52:8020/models/CV.model\")\n",
    "# 得出词频向量结果\n",
    "cv_result = cv_model.transform(words_df)\n",
    "# 训练IDF模型\n",
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF(inputCol=\"countFeatures\", outputCol=\"idfFeatures\")\n",
    "idfModel = idf.fit(cv_result)\n",
    "idfModel.write().overwrite().save(\"hdfs://192.168.0.52:8020/models/IDF.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37702786, 0.61473956, 0.50244703, 0.63056421, 0.72375587,\n",
       "       0.61016019, 0.68230305, 0.70786613, 0.62147479, 0.64961663,\n",
       "       0.63001098, 0.94416523, 0.84821951, 1.17245272, 0.99504718,\n",
       "       0.77097374, 0.96936827, 1.35487223, 0.73713712, 1.69034085])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfModel.idf.toArray()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------------------+--------------------+\n",
      "|          article_id|channel_name|                         words|       countFeatures|\n",
      "+--------------------+------------+------------------------------+--------------------+\n",
      "|00000982fda1948f4...|      投资界|[汽车, 新能源, 汽车, 投资界...|(572577,[0,1,2,3,...|\n",
      "|000039bca765fca66...|      猎云网| [罗永浩, 锤子, 科技, 广益,...|(572577,[0,1,2,5,...|\n",
      "|0000b6f5182e097e8...|      砍柴网|[特斯拉, 二手车, 业务, 砍柴...|(572577,[0,1,2,12...|\n",
      "|0000bca75b0386b06...|      希鸥网|      [unknow, 希鸥, 用户, ...|(572577,[0,1,2,3,...|\n",
      "|0000f7b314fe7970d...|        36氪|     [unknow, 谷歌, 母公司,...|(572577,[0,4,5,6,...|\n",
      "|00019c1868a4ae670...|      投资界|[雅居乐, 雅尊会, 投资界, 经...|(572577,[1,3,7,13...|\n",
      "|0001a643f8a623a0c...|      希鸥网|      [unknow, 希鸥, 希鸥, ...|(572577,[0,1,2,3,...|\n",
      "|0001c67ae9c818e47...|      猎云网|      [Journal, 信息, 工具,...|(572577,[0,4,5,7,...|\n",
      "|0002d00a0273e73c8...|        亿欧|[公立医院, 医生, 执业, 黄石...|(572577,[0,1,2,3,...|\n",
      "|000314168bdf88130...|      投资界|  [智能, 家具, 系统, 宜家, ...|(572577,[0,1,2,3,...|\n",
      "|00033465e6868fcbf...|        亿欧|  [苏宁, 物流, 苏宁, 网购, ...|(572577,[0,4,5,6,...|\n",
      "|0003d40b95a019c7d...|      投资界|[基金, 投资界, 投资界, 快讯...|(572577,[0,1,2,3,...|\n",
      "|00047e8d4942862a8...| 新芽NewSeed|    [人工智能, 新芽, NewSee...|(572577,[0,1,2,4,...|\n",
      "|00049e1b615f017e9...|        亿欧|[小冰, 沈向洋, 人工智能, 图...|(572577,[0,2,3,4,...|\n",
      "|0004f0d046da399e6...|        亿欧|[无人驾驶, 智慧, 物流, 技术...|(572577,[0,2,3,5,...|\n",
      "|0004f297f3e596aa0...|        亿欧|[编程, 奥数, 素质教育, 公司...|(572577,[0,2,3,4,...|\n",
      "|0005bc35146c88a95...|        亿欧|[宁德, 时代, 比亚迪, 动力电...|(572577,[0,1,2,3,...|\n",
      "|000676ddec6a8b1eb...|        36氪|        [unknow, 以太, PoS,...|(572577,[0,2,4,5,...|\n",
      "|0006d4a4940e140ec...|        36氪|         [unknow, B2B2C, 模...|(572577,[1,2,4,5,...|\n",
      "|000782d1d98bc6827...|        亿欧|   [特斯拉, ofo, 能源, 比亚...|(572577,[0,2,13,2...|\n",
      "+--------------------+------------+------------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_res = idfModel.transform(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------------------+--------------------+--------------------+\n",
      "|          article_id|channel_name|                         words|       countFeatures|         idfFeatures|\n",
      "+--------------------+------------+------------------------------+--------------------+--------------------+\n",
      "|00000982fda1948f4...|      投资界|[汽车, 新能源, 汽车, 投资界...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|000039bca765fca66...|      猎云网| [罗永浩, 锤子, 科技, 广益,...|(572577,[0,1,2,5,...|(572577,[0,1,2,5,...|\n",
      "|0000b6f5182e097e8...|      砍柴网|[特斯拉, 二手车, 业务, 砍柴...|(572577,[0,1,2,12...|(572577,[0,1,2,12...|\n",
      "|0000bca75b0386b06...|      希鸥网|      [unknow, 希鸥, 用户, ...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|0000f7b314fe7970d...|        36氪|     [unknow, 谷歌, 母公司,...|(572577,[0,4,5,6,...|(572577,[0,4,5,6,...|\n",
      "|00019c1868a4ae670...|      投资界|[雅居乐, 雅尊会, 投资界, 经...|(572577,[1,3,7,13...|(572577,[1,3,7,13...|\n",
      "|0001a643f8a623a0c...|      希鸥网|      [unknow, 希鸥, 希鸥, ...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|0001c67ae9c818e47...|      猎云网|      [Journal, 信息, 工具,...|(572577,[0,4,5,7,...|(572577,[0,4,5,7,...|\n",
      "|0002d00a0273e73c8...|        亿欧|[公立医院, 医生, 执业, 黄石...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|000314168bdf88130...|      投资界|  [智能, 家具, 系统, 宜家, ...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|00033465e6868fcbf...|        亿欧|  [苏宁, 物流, 苏宁, 网购, ...|(572577,[0,4,5,6,...|(572577,[0,4,5,6,...|\n",
      "|0003d40b95a019c7d...|      投资界|[基金, 投资界, 投资界, 快讯...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|00047e8d4942862a8...| 新芽NewSeed|    [人工智能, 新芽, NewSee...|(572577,[0,1,2,4,...|(572577,[0,1,2,4,...|\n",
      "|00049e1b615f017e9...|        亿欧|[小冰, 沈向洋, 人工智能, 图...|(572577,[0,2,3,4,...|(572577,[0,2,3,4,...|\n",
      "|0004f0d046da399e6...|        亿欧|[无人驾驶, 智慧, 物流, 技术...|(572577,[0,2,3,5,...|(572577,[0,2,3,5,...|\n",
      "|0004f297f3e596aa0...|        亿欧|[编程, 奥数, 素质教育, 公司...|(572577,[0,2,3,4,...|(572577,[0,2,3,4,...|\n",
      "|0005bc35146c88a95...|        亿欧|[宁德, 时代, 比亚迪, 动力电...|(572577,[0,1,2,3,...|(572577,[0,1,2,3,...|\n",
      "|000676ddec6a8b1eb...|        36氪|        [unknow, 以太, PoS,...|(572577,[0,2,4,5,...|(572577,[0,2,4,5,...|\n",
      "|0006d4a4940e140ec...|        36氪|         [unknow, B2B2C, 模...|(572577,[1,2,4,5,...|(572577,[1,2,4,5,...|\n",
      "|000782d1d98bc6827...|        亿欧|   [特斯拉, ofo, 能源, 比亚...|(572577,[0,2,13,2...|(572577,[0,2,13,2...|\n",
      "+--------------------+------------+------------------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_with_idf = list(zip(cv_model.vocabulary, idfModel.idf.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data):\n",
    "    for index in range(len(data)):\n",
    "        data[index] = list(data[index])\n",
    "        data[index].append(index)\n",
    "        data[index][1] = float(data[index][1])\n",
    "func(keywords_list_with_idf)\n",
    "sc = ktt.spark.sparkContext\n",
    "rdd = sc.parallelize(keywords_list_with_idf)\n",
    "df = rdd.toDF([\"keywords\", \"idf\", \"index\"])\n",
    "df.write.insertInto('idf_keywords_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktt.spark.sql(\"use article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+\n",
      "|keyword|               idf|index|\n",
      "+-------+------------------+-----+\n",
      "|   劲邦| 9.091431221721468|71680|\n",
      "|   项兵| 9.576939037503168|71681|\n",
      "|   龙巅| 11.25091547107484|71682|\n",
      "|   SoIC|10.403617610687636|71683|\n",
      "| Ginger| 9.305005322019527|71684|\n",
      "| 市议员| 9.130651934874749|71685|\n",
      "|   手力| 8.982231929756477|71686|\n",
      "| fitbit| 9.305005322019527|71687|\n",
      "|   终章| 9.017323249567745|71688|\n",
      "| 罗忠生|  9.71047043012769|71689|\n",
      "+-------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql(\"select * from idf_keywords_values limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+\n",
      "|col_name|data_type|   comment|\n",
      "+--------+---------+----------+\n",
      "| keyword|   string|article_id|\n",
      "|     idf|   double|       idf|\n",
      "|   index|      int|     index|\n",
      "+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql(\" desc idf_keywords_values\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(partition):\n",
    "    TOPK = 20\n",
    "    for row in partition:\n",
    "        # 找到索引与IDF值并进行排序\n",
    "        _ = list(zip(row.idfFeatures.indices, row.idfFeatures.values))\n",
    "        _ = sorted(_, key=lambda x: x[1], reverse=True)\n",
    "        result = _[:TOPK]\n",
    "        for word_index, tfidf in result:\n",
    "            yield row.article_id, row.channel_name, int(word_index), round(float(tfidf), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_keywordsByTFIDF = tfidf_res.rdd.mapPartitions(func).toDF([\"article_id\", \"channel_name\", \"index\", \"tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "_keywordsByTFIDF.write.insertInto(\"pre_tfidf_keywords_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|keyword|idx|\n",
      "+-------+---+\n",
      "|   公司|  0|\n",
      "|   企业|  1|\n",
      "|   市场|  2|\n",
      "|   中国|  3|\n",
      "|   用户|  4|\n",
      "|   产品|  5|\n",
      "|   技术|  6|\n",
      "|   平台|  7|\n",
      "|   行业|  8|\n",
      "|   数据|  9|\n",
      "|   问题| 10|\n",
      "| 互联网| 11|\n",
      "|   业务| 12|\n",
      "|   品牌| 13|\n",
      "|   科技| 14|\n",
      "|   领域| 15|\n",
      "|   内容| 16|\n",
      "|   智能| 17|\n",
      "|   时间| 18|\n",
      "|   腾讯| 19|\n",
      "+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywordsIndex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsIndex = ktt.spark.sql(\"select keyword, index idx from idf_keywords_values\")\n",
    "keywordsByTFIDF = _keywordsByTFIDF.join(keywordsIndex, keywordsIndex.idx == _keywordsByTFIDF.index).select([\"article_id\", \"channel_name\", \"keyword\", \"tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------+-------+\n",
      "|          article_id|channel_name|keyword|  tfidf|\n",
      "+--------------------+------------+-------+-------+\n",
      "|0afd1b0d983d1c257...|        36氪|   全球| 1.0243|\n",
      "|0b2009b79fb2eb230...|        36氪|   全球| 5.1217|\n",
      "|0b4e4c2254c54a914...|      品途网|   全球|22.5355|\n",
      "|0b57598f2de7f168c...|        亿欧|   全球|15.3651|\n",
      "|0b5e51cdabe3d6052...|        亿欧|   全球|12.2921|\n",
      "|0b7143b66475c6d48...|      品途网|   全球|15.3651|\n",
      "|0b7af754981dbba09...|      投资界|   全球|13.3165|\n",
      "|0ba08990543798a16...|      品途网|   全球|15.3651|\n",
      "|0bb7238085dbc11ce...|    IT时代网|   全球|17.4138|\n",
      "|0bbb69d12fe2fa4a6...|      DoNews|   全球| 7.1704|\n",
      "|0bd19c2d53e8dc7d0...|    IT时代网|   全球| 9.2191|\n",
      "|0bf34838372b3bb82...|      品途网|   全球|15.3651|\n",
      "|0c331b2c6d47aa4d0...|      IT之家|   全球| 9.2191|\n",
      "|0c47d47456d2e76df...|      鞭牛士|   全球|12.2921|\n",
      "|0c4f9d99965c834db...|      DoNews|   全球| 2.0487|\n",
      "|0c54f8a36ad23b53c...|      品途网|   全球|14.3408|\n",
      "|0c694ba7d9f0b88c6...|      砍柴网|   全球|20.4869|\n",
      "|0c72b2f824ff58b30...|        亿欧|   全球|10.2434|\n",
      "|0c8d4f6b9f9101982...|  世界经理人|   全球| 8.1947|\n",
      "|0ceb6f886bcb4f519...| 新芽NewSeed|   全球|16.3895|\n",
      "+--------------------+------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywordsByTFIDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywordsByTFIDF.write.insertInto(\"tfidf_keywords_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+\n",
      "|    col_name|data_type|     comment|\n",
      "+------------+---------+------------+\n",
      "|  article_id|   string|  article_id|\n",
      "|channel_name|   string|channel_name|\n",
      "|     keyword|   string|     keyword|\n",
      "|       tfidf|   double|       tfidf|\n",
      "|article_time|   string|        null|\n",
      "+------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql(\"desc tfidf_keywords_values\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 4870934|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql(\"select count(*) from tfidf_keywords_values \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+-------+-------------------+\n",
      "|          article_id|channel_name| keyword|  tfidf|       article_time|\n",
      "+--------------------+------------+--------+-------+-------------------+\n",
      "|2701f73f7c9437176...|        36氪|阿里巴巴| 9.2886|2019-12-11 08:31:05|\n",
      "|6c2f9195ecc86cf94...|      IT之家|    信用| 3.1126|2019-12-11 08:24:34|\n",
      "|64cd91c52de06503e...|        36氪|     app|15.3317|2019-12-11 08:06:05|\n",
      "|efb302b2dbbc29c20...|        亿欧|    低温|45.3093|2019-12-11 08:58:36|\n",
      "|d7798752d5f82aad1...|      猎云网|    复星|29.1006|2019-12-10 19:58:11|\n",
      "|d7798752d5f82aad1...|      猎云网|    复星|29.1006|2019-12-10 19:58:11|\n",
      "|d7798752d5f82aad1...|      猎云网|    复星|29.1006|2019-12-10 19:58:11|\n",
      "|d7798752d5f82aad1...|      猎云网|    复星|29.1006|2019-12-10 19:58:11|\n",
      "|d7798752d5f82aad1...|      猎云网|    复星|29.1006|2019-12-10 19:58:11|\n",
      "|d7798752d5f82aad1...|      猎云网|    复星|29.1006|2019-12-10 19:58:11|\n",
      "+--------------------+------------+--------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql(\"select * from tfidf_keywords_values limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tags = ktt.spark.sql(\"select article_id as article_id2, channel_name as channel_name2, source_tags, article_time from article_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_keywords  = ktt.spark.sql(\"select article_id, min(channel_name) channel_name, collect_list(keyword) keywords, collect_list(tfidf) weights from tfidf_keywords_values group by article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords_info = merge_keywords.join(source_tags, merge_keywords.article_id == source_tags.article_id2).select([\"article_id\", \"channel_name\", \"keywords\", \"weights\",\"source_tags\",\"article_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(article_id,StringType,true),StructField(channel_name,StringType,true),StructField(keywords,ArrayType(StringType,true),true),StructField(weights,ArrayType(DoubleType,true),true),StructField(source_tags,StringType,true),StructField(article_time,StringType,true)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_info.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\", StringType())\n",
    "def change_unknow_udf(keywords, source_tags):\n",
    "    if source_tags == \"unknow\":\n",
    "        source_tags = keywords\n",
    "    elif source_tags == None :\n",
    "        source_tags = keywords\n",
    "    elif len(source_tags) <=3:\n",
    "        source_tags = keywords\n",
    "    return source_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_info = keywords_info.withColumn(\"topics\", change_unknow_udf(keywords_info.keywords, keywords_info.source_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------------------+--------------------+----------------------------+-------------------+------------------------------+\n",
      "|          article_id|channel_name|                      keywords|             weights|                 source_tags|       article_time|                        topics|\n",
      "+--------------------+------------+------------------------------+--------------------+----------------------------+-------------------+------------------------------+\n",
      "|00a32656f423c19bd...|        36氪| [信心, 负面, 目标, 转化率,...|[19.6395, 24.0735...|                        null|2019-06-27 20:31:05| [信心, 负面, 目标, 转化率,...|\n",
      "|00a77a605fa1f8307...|        亿欧|[板块, 学校, 普华永道, 香港...|[34.4602, 15.7381...|    [ \"教育\" , \"政策\" , \"...|2019-01-21 18:30:00|      [ \"教育\" , \"政策\" , \"...|\n",
      "|0167a23fb8747af19...|        36氪|[上柜, 仿真技术, 折纸, 科技...|[9.305, 7.64, 8.3...|                        null|2019-03-14 13:44:56|[上柜, 仿真技术, 折纸, 科技...|\n",
      "|016ba612ee61ec97f...|      DoNews|          [K11, Club, Party...|[7.0265, 17.2777,...|                        null|2019-07-17 11:55:53|          [K11, Club, Party...|\n",
      "|01abaff78180a689a...|      投资界| [转型, 企业主, 成本, 订单,...|[16.6874, 25.959,...|    [ \"海尔\" , \"蒙牛\" , \"...|2018-12-27 10:46:00|      [ \"海尔\" , \"蒙牛\" , \"...|\n",
      "|01ad671ef2b30d864...|  世界经理人|[人员, 战略目标, 经理人, 大...|[22.047, 14.3793,...|    [ \"战略\" , \"绩效\" , \"...|2019-03-24 00:00:00|      [ \"战略\" , \"绩效\" , \"...|\n",
      "|01fc229aad630a827...|      鞭牛士|  [酷狗, 权限, 属地, 通报, ...|[11.7267, 4.0537,...|               [ \"用户信息\"]|2019-07-05 15:54:00|                 [ \"用户信息\"]|\n",
      "|0201180aa7a6ea0b0...|        36氪|[断物, 传教士, 公司, 创业者...|[52.7888, 23.0022...|                        null|2019-03-17 11:01:14|[断物, 传教士, 公司, 创业者...|\n",
      "|020d506efcf0cb0d7...|        36氪|  [视频, 公众, 京东, 手机, ...|[1.7426, 1.6696, ...|                        null|2018-11-14 04:08:16|  [视频, 公众, 京东, 手机, ...|\n",
      "|02200c638f14b13b8...|    站长之家| [墨蓝新, 旗舰, 手机, 前脚,...|[10.9632, 15.5345...|[ \"小米手机\" , \"滑板电动车\"]|2018-07-11 13:39:00|  [ \"小米手机\" , \"滑板电动车\"]|\n",
      "|0220282294ecd6ad3...|  世界经理人|      [城市, 院校, referrer...|[18.9002, 19.6258...|      [ \"工作\" , \"职场攻略\"]|2018-10-29 00:00:00|        [ \"工作\" , \"职场攻略\"]|\n",
      "|02a3eae81d9b6a485...|      DoNews|[事情, 股价, 王怀南, 宝树王...|[8.0344, 25.6527,...|                        null|2019-09-27 12:45:01|[事情, 股价, 王怀南, 宝树王...|\n",
      "|02d0ea03a3650836d...|      砍柴网|    [洽谈会, 首钢, Beat, 大...|[15.9102, 13.7129...|                     [ \"VR\"]|2018-09-07 13:03:00|                       [ \"VR\"]|\n",
      "|0308ec4475a99dc4e...|        36氪|   [Maw, 电子邮件, 门店, 邮...|[9.8646, 8.6559, ...|                        null|2018-11-14 04:08:17|   [Maw, 电子邮件, 门店, 邮...|\n",
      "|030e4b266bf1db7f5...|      IT之家|[书记, 退党, 中国移动, 总经...|[10.4809, 11.2509...|                        null|2019-09-30 08:20:01|[书记, 退党, 中国移动, 总经...|\n",
      "|0387e5783ce27d036...|        36氪|  [河西, 人所, 秦军, 魏国, ...|[6.6558, 8.457699...|                        null|2019-07-09 19:19:38|  [河西, 人所, 秦军, 魏国, ...|\n",
      "|039909c1bb63bd620...|      猎云网| [机器人, 教会, 程序, 摩望,...|[20.109, 5.2462, ...|    [ \"机器学习\" , \"英伟达\"]|2018-05-22 08:55:48|      [ \"机器学习\" , \"英伟达\"]|\n",
      "|03f21a15d38a8a802...|      DoNews|        [IPO, CDR, 香港, Do...|[10.6189, 11.3805...|                        null|2018-06-19 16:52:58|        [IPO, CDR, 香港, Do...|\n",
      "|04271469b23c4deb8...|  世界经理人|        [KENTEX, 买方, KACO...|[11.6564, 21.0814...|[ \"品牌定位\" , \"市场营销\"...|2019-06-27 00:00:00|  [ \"品牌定位\" , \"市场营销\"...|\n",
      "|0441af97ffea154ae...|      IT之家|  [大会, 转型, 生态, 官宣, ...|[9.0595, 3.7083, ...|                        null|2019-11-05 19:42:02|  [大会, 转型, 生态, 官宣, ...|\n",
      "+--------------------+------------+------------------------------+--------------------+----------------------------+-------------------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _func(row):\n",
    "    arry_topics = row.topics.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\\"\",\"\").replace(\" \",\"\").split(\",\")\n",
    "    return row.article_id, row.channel_name, dict(zip(row.keywords, row.weights)), arry_topics, row.article_time\n",
    "article_profile = keywords_info.rdd.map(_func).toDF([\"article_id\", \"channel_name\", \"keywords\", \"topics\",\"article_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_profile.schame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`topics`' due to data type mismatch: cannot cast string to array<string>;;\\n'Project [article_id#314, channel_name#315, keywords#316, cast(topics#317 as array<string>) AS topics#375, article_time#318]\\n+- LogicalRDD [article_id#314, channel_name#315, keywords#316, topics#317, article_time#318], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/rec_sys/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1580995/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o436.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`topics`' due to data type mismatch: cannot cast string to array<string>;;\n'Project [article_id#314, channel_name#315, keywords#316, cast(topics#317 as array<string>) AS topics#375, article_time#318]\n+- LogicalRDD [article_id#314, channel_name#315, keywords#316, topics#317, article_time#318], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2252)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2219)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-be9d560b3a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marticle_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/rec_sys/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \"\"\"\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1580995/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/rec_sys/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`topics`' due to data type mismatch: cannot cast string to array<string>;;\\n'Project [article_id#314, channel_name#315, keywords#316, cast(topics#317 as array<string>) AS topics#375, article_time#318]\\n+- LogicalRDD [article_id#314, channel_name#315, keywords#316, topics#317, article_time#318], false\\n\""
     ]
    }
   ],
   "source": [
    "article_profile = article_profile.withColumn(\"topics\", article_profile.topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(article_id,StringType,true),StructField(channel_name,StringType,true),StructField(keywords,MapType(StringType,DoubleType,true),true),StructField(topics,StringType,true),StructField(article_time,StringType,true)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_profile.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`topics`' due to data type mismatch: cannot cast string to array<string>;;\\n'InsertIntoHiveTable `article`.`article_profile`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, false, false, [article_id, channel_name, keyword, topics, article_time]\\n+- 'Project [cast(article_id#314 as string) AS article_id#370, cast(channel_name#315 as string) AS channel_name#371, cast(keywords#316 as map<string,double>) AS keyword#372, cast(topics#317 as array<string>) AS topics#373, article_time#318]\\n   +- LogicalRDD [article_id#314, channel_name#315, keywords#316, topics#317, article_time#318], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/rec_sys/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1580995/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o438.insertInto.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`topics`' due to data type mismatch: cannot cast string to array<string>;;\n'InsertIntoHiveTable `article`.`article_profile`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, false, false, [article_id, channel_name, keyword, topics, article_time]\n+- 'Project [cast(article_id#314 as string) AS article_id#370, cast(channel_name#315 as string) AS channel_name#371, cast(keywords#316 as map<string,double>) AS keyword#372, cast(topics#317 as array<string>) AS topics#373, article_time#318]\n   +- LogicalRDD [article_id#314, channel_name#315, keywords#316, topics#317, article_time#318], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:61)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:60)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:76)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:325)\n\tat org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:311)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cf2004927791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marticle_profile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"article_profile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/rec_sys/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36minsertInto\u001b[0;34m(self, tableName, overwrite)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mOptionally\u001b[0m \u001b[0moverwriting\u001b[0m \u001b[0many\u001b[0m \u001b[0mexisting\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1580995/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/rec_sys/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`topics`' due to data type mismatch: cannot cast string to array<string>;;\\n'InsertIntoHiveTable `article`.`article_profile`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, false, false, [article_id, channel_name, keyword, topics, article_time]\\n+- 'Project [cast(article_id#314 as string) AS article_id#370, cast(channel_name#315 as string) AS channel_name#371, cast(keywords#316 as map<string,double>) AS keyword#372, cast(topics#317 as array<string>) AS topics#373, article_time#318]\\n   +- LogicalRDD [article_id#314, channel_name#315, keywords#316, topics#317, article_time#318], false\\n\""
     ]
    }
   ],
   "source": [
    "article_profile.write.insertInto(\"article_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------------+------------------------------+\n",
      "|          article_id|channel_name|                 keywords|                        topics|\n",
      "+--------------------+------------+-------------------------+------------------------------+\n",
      "|00a32656f423c19bd...|        36氪|[波斯 -> 31.87, 讲故事...|[销售额, 讲故事, 信心, 目标...|\n",
      "|00a77a605fa1f8307...|        亿欧|   [A股 -> 30.4946, 庞...|      [ \"教育\" , \"政策\" , \"...|\n",
      "|0167a23fb8747af19...|        36氪|  [科技 -> 11.9406, 智...|  [助力, 场景, 画像, 杨恒, ...|\n",
      "|016ba612ee61ec97f...|      DoNews|  [文化展 -> 9.4051, B...|    [体验, 芭比, K11, 布朗,...|\n",
      "|01abaff78180a689a...|      投资界|  [大脑 -> 12.9711, 订...|      [ \"海尔\" , \"蒙牛\" , \"...|\n",
      "|01ad671ef2b30d864...|  世界经理人| [单兵作战 -> 13.5171,...|      [ \"战略\" , \"绩效\" , \"...|\n",
      "|01fc229aad630a827...|      鞭牛士|   [权限 -> 4.0537, Ap...|                 [ \"用户信息\"]|\n",
      "|0201180aa7a6ea0b0...|        36氪|  [效应 -> 30.7139, 战...|     [效应, Leader, 延展性,...|\n",
      "|020d506efcf0cb0d7...|        36氪|    [App -> 5.3512, 公...|    [PLUS, 公众, 编者按, 京...|\n",
      "|02200c638f14b13b8...|    站长之家|  [滑板 -> 19.7249, 手...|  [ \"小米手机\" , \"滑板电动车\"]|\n",
      "|0220282294ecd6ad3...|  世界经理人|  [人群 -> 18.3445, 图...|        [ \"工作\" , \"职场攻略\"]|\n",
      "|02a3eae81d9b6a485...|      DoNews|     [Tang -> 8.1154, ...|  [员工, 团队, 大家, 裁员, ...|\n",
      "|02d0ea03a3650836d...|      砍柴网|   [大脑 -> 12.9711, T...|                       [ \"VR\"]|\n",
      "|0308ec4475a99dc4e...|        36氪|    [Maw -> 9.8646, 通...|       [回头客, Rewards, St...|\n",
      "|030e4b266bf1db7f5...|      IT之家|  [书记 -> 10.4809, 党...|[天津, 天津大学, 退党, 广大...|\n",
      "|0387e5783ce27d036...|        36氪| [河西 -> 6.6558, 秦孝...| [部队, 魏军, 秦孝公, 赵国,...|\n",
      "|039909c1bb63bd620...|      猎云网| [机器人 -> 20.109, 序...|      [ \"机器学习\" , \"英伟达\"]|\n",
      "|03f21a15d38a8a802...|      DoNews| [计划 -> 2.4678, 文件...|        [CDR, DoNews6, 人士...|\n",
      "|04271469b23c4deb8...|  世界经理人|   [买方 -> 21.0814, K...|  [ \"品牌定位\" , \"市场营销\"...|\n",
      "|0441af97ffea154ae...|      IT之家| [官宣 -> 5.3347, 时代...|[数字化, 华为, 创新能力, 关...|\n",
      "+--------------------+------------+-------------------------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------------------+-----------------------------+\n",
      "|          article_id|channel_name|                keyword|                       topics|\n",
      "+--------------------+------------+-----------------------+-----------------------------+\n",
      "|00a32656f423c19bd...|        36氪|[讲故事 -> 18.5448, ...|[对象, 情感, 时候, 转化率,...|\n",
      "+--------------------+------------+-----------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ktt.spark.sql('select * from article_profile limit 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
